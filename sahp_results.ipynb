{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T03:44:51.305832Z",
     "start_time": "2021-07-28T03:44:51.287870Z"
    }
   },
   "outputs": [],
   "source": [
    "from train_functions import train_sahp\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from utils.load_synth_data import process_loaded_sequences\n",
    "from utils.util import get_batch,count_parameters\n",
    "from utils.atten_optimizer import NoamOpt\n",
    "from train_functions.train_sahp import make_model,eval_sahp,prediction_evaluation,MaskBatch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synth Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T21:48:55.225745Z",
     "start_time": "2021-06-23T21:48:54.599985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = train_sahp.make_model(max_sequence_length=324)\n",
    "model_dict =torch.load('saved_models/sahp-synthetic_hidden16-20210622-205430',map_location=torch.device('cpu'))\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T20:28:04.193357Z",
     "start_time": "2021-07-27T20:28:04.089550Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/simulated/hawkes_synthetic_random_2d_20191130-180837.pkl', 'rb') as f:\n",
    "    loaded_hawkes_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T20:28:41.399885Z",
     "start_time": "2021-07-27T20:28:41.381245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_hawkes_data['types'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T20:28:23.625305Z",
     "start_time": "2021-07-27T20:28:23.612340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.60763443,   6.10909592,   8.14957647,   8.34786877,\n",
       "        11.74765717,  12.09120782,  12.7038931 ,  12.92263578,\n",
       "        13.27106319,  13.9827004 ,  14.40075975,  14.40657523,\n",
       "        15.65816619,  17.47021075,  22.1415383 ,  22.18185583,\n",
       "        22.25583239,  25.19507215,  25.72558125,  26.73629369,\n",
       "        27.66141843,  29.90981069,  30.74234429,  37.87818156,\n",
       "        46.43551261,  46.92593361,  47.68628126,  48.4036156 ,\n",
       "        49.27436552,  50.98216357,  51.41173408,  51.41306198,\n",
       "        53.30754214,  54.47245528,  55.07976453,  55.58598659,\n",
       "        55.60622739,  55.60733836,  55.79603789,  56.18765199,\n",
       "        56.71815713,  57.69584592,  58.44575486,  58.44984819,\n",
       "        59.61075839,  59.83157271,  59.91939916,  64.38615531,\n",
       "        64.44525006,  65.45187781,  65.79173705,  67.33287862,\n",
       "        68.40510542,  68.70232507,  69.15380095,  69.44215237,\n",
       "        69.4541937 ,  69.51216464,  69.54023449,  70.02401056,\n",
       "        70.16574478,  71.1551833 ,  71.27132983,  73.8784023 ,\n",
       "        75.58811262,  76.95664707,  77.52380695,  83.06858246,\n",
       "        84.19643217,  86.30583849,  87.42048135,  88.57546455,\n",
       "        89.6870725 ,  90.68697834,  94.59844555,  94.97760338,\n",
       "        95.47432676,  97.28672202,  98.96442676,  99.15217268,\n",
       "        99.72004225, 100.51851029, 101.85568578, 103.04427581,\n",
       "       103.5856367 , 104.62929432, 105.44087946, 108.52210955,\n",
       "       108.94138396, 109.16444043, 109.36959848, 109.58325049,\n",
       "       110.4577918 , 110.46118131, 110.85596392, 113.16428461,\n",
       "       114.20694372, 115.59973419, 122.33626989, 126.01318802,\n",
       "       126.98374217, 130.32923412, 145.09742342, 145.91809077,\n",
       "       145.96235452, 147.37427282, 147.95942551, 152.54161579,\n",
       "       164.51685125, 165.03511665, 166.36321634, 166.72644   ,\n",
       "       167.1745074 , 167.66313029, 168.61509429, 168.80288235,\n",
       "       172.46356363, 173.07862423, 176.21046257, 179.55822806,\n",
       "       180.37830757, 181.92538541, 183.83513104, 186.51910738,\n",
       "       186.56545865, 188.93905933, 189.2467037 , 199.6974972 ,\n",
       "       199.82917419, 199.85117545])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_hawkes_data['timestamps'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T21:49:11.826828Z",
     "start_time": "2021-06-23T21:48:55.227709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sequence_length: 323\n"
     ]
    }
   ],
   "source": [
    "## Load Data Set\n",
    "with open('data/simulated/hawkes_synthetic_random_2d_20191130-180837.pkl', 'rb') as f:\n",
    "    loaded_hawkes_data = pickle.load(f)\n",
    "\n",
    "tmax = loaded_hawkes_data['tmax']\n",
    "\n",
    "seq_times, seq_types, seq_lengths, _ = process_loaded_sequences(loaded_hawkes_data, 2)\n",
    "\n",
    "total_sample_size = seq_times.size(0)\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * total_sample_size)\n",
    "dev_ratio =0.1\n",
    "dev_size = int(dev_ratio * total_sample_size)\n",
    "## Split Traning and Test Sets\n",
    "train_seq_times = seq_times[:train_size]\n",
    "train_seq_types = seq_types[:train_size]\n",
    "train_seq_lengths = seq_lengths[:train_size]\n",
    "\n",
    "\n",
    "dev_seq_times = seq_times[train_size:train_size + dev_size]  # train_size+dev_size\n",
    "dev_seq_types = seq_types[train_size:train_size + dev_size]\n",
    "dev_seq_lengths = seq_lengths[train_size:train_size + dev_size]\n",
    "\n",
    "test_seq_times = seq_times[-dev_size:]\n",
    "test_seq_types = seq_types[-dev_size:]\n",
    "test_seq_lengths = seq_lengths[-dev_size:]\n",
    "\n",
    "\n",
    "## sequence length\n",
    "train_seq_lengths, reorder_indices_train = train_seq_lengths.sort(descending=True)\n",
    "# # Reorder by descending sequence length\n",
    "train_seq_times = train_seq_times[reorder_indices_train]\n",
    "train_seq_types = train_seq_types[reorder_indices_train]\n",
    "#\n",
    "dev_seq_lengths, reorder_indices_dev = dev_seq_lengths.sort(descending=True)\n",
    "# # Reorder by descending sequence length\n",
    "dev_seq_times = dev_seq_times[reorder_indices_dev]\n",
    "dev_seq_types = dev_seq_types[reorder_indices_dev]\n",
    "\n",
    "test_seq_lengths, reorder_indices_test = test_seq_lengths.sort(descending=True)\n",
    "# # Reorder by descending sequence length\n",
    "test_seq_times = test_seq_times[reorder_indices_test]\n",
    "test_seq_types = test_seq_types[reorder_indices_test]\n",
    "\n",
    "max_sequence_length = max(train_seq_lengths[0], dev_seq_lengths[0], test_seq_lengths[0])\n",
    "print('max_sequence_length: {}'.format(max_sequence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T21:49:41.137963Z",
     "start_time": "2021-06-23T21:49:11.828827Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [00:16<00:00, 23.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 17.319760118898063\n",
      "Type prediction score: 0.5575\n",
      "tensor(0.6343)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 32\n",
    "test_size = test_seq_times.size(0)\n",
    "device = 'cpu'\n",
    "test_loop_range = list(range(0, test_size, batch_size))\n",
    "\n",
    "## Get test Loss\n",
    "test_event_num, epoch_test_loss = eval_sahp(batch_size, test_loop_range, test_seq_lengths, test_seq_times,\n",
    "                                            test_seq_types, model, device, 0)\n",
    "\n",
    "test_loss = epoch_test_loss/test_event_num\n",
    "\n",
    "## Get Result Metrics\n",
    "avg_rmse, types_predict_score, results = prediction_evaluation(\n",
    "    device, model, test_seq_lengths, test_seq_times, test_seq_types, test_size, tmax)\n",
    "\n",
    "print(test_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T22:05:35.767157Z",
     "start_time": "2021-06-23T22:04:35.091720Z"
    }
   },
   "outputs": [],
   "source": [
    "## Load Data Set\n",
    "dataset = 'retweet'\n",
    "process_dim =3 \n",
    "\n",
    "train_path = 'data/' + dataset + '/train_manifold_format.pkl'\n",
    "dev_path = 'data/' + dataset + '/dev_manifold_format.pkl'\n",
    "test_path = 'data/' + dataset + '/test_manifold_format.pkl'\n",
    "\n",
    "with open(train_path, 'rb') as f:\n",
    "    train_hawkes_data = pickle.load(f)\n",
    "with open(dev_path, 'rb') as f:\n",
    "    dev_hawkes_data = pickle.load(f)\n",
    "with open(test_path, 'rb') as f:\n",
    "    test_hawkes_data = pickle.load(f)\n",
    "\n",
    "train_seq_times, train_seq_types, train_seq_lengths, train_tmax = \\\n",
    "process_loaded_sequences(train_hawkes_data, process_dim)\n",
    "dev_seq_times, dev_seq_types, dev_seq_lengths, dev_tmax = \\\n",
    "process_loaded_sequences(dev_hawkes_data, process_dim)\n",
    "test_seq_times, test_seq_types, test_seq_lengths, test_tmax = \\\n",
    "process_loaded_sequences(test_hawkes_data, process_dim)\n",
    "\n",
    "tmax = max([train_tmax, dev_tmax, test_tmax])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T22:07:33.676349Z",
     "start_time": "2021-06-23T22:07:33.642195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample size: 20000\n",
      "Dev sample size: 2000\n",
      "Test sample size: 2000\n",
      "No. of event tokens in training subset: tensor(2176116)\n",
      "No. of event tokens in development subset: tensor(215521)\n",
      "No. of event tokens in test subset: tensor(218465)\n",
      "max_sequence_length: 264\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "train_sample_size = train_seq_times.size(0)\n",
    "print(\"Train sample size: {}\".format(train_sample_size))\n",
    "\n",
    "dev_sample_size = dev_seq_times.size(0)\n",
    "print(\"Dev sample size: {}\".format(dev_sample_size))\n",
    "\n",
    "test_sample_size = test_seq_times.size(0)\n",
    "print(\"Test sample size: {}\".format(test_sample_size))\n",
    "\n",
    "\n",
    "# Define training data\n",
    "train_seq_times = train_seq_times.to(device)\n",
    "train_seq_types = train_seq_types.to(device)\n",
    "train_seq_lengths = train_seq_lengths.to(device)\n",
    "print(\"No. of event tokens in training subset:\", train_seq_lengths.sum())\n",
    "\n",
    "# Define development data\n",
    "dev_seq_times = dev_seq_times.to(device)\n",
    "dev_seq_types = dev_seq_types.to(device)\n",
    "dev_seq_lengths = dev_seq_lengths.to(device)\n",
    "print(\"No. of event tokens in development subset:\", dev_seq_lengths.sum())\n",
    "\n",
    "# Define test data\n",
    "test_seq_times = test_seq_times.to(device)\n",
    "test_seq_types = test_seq_types.to(device)\n",
    "test_seq_lengths = test_seq_lengths.to(device)\n",
    "print(\"No. of event tokens in test subset:\", test_seq_lengths.sum())\n",
    "\n",
    "\n",
    "## sequence length\n",
    "train_seq_lengths, reorder_indices_train = train_seq_lengths.sort(descending=True)\n",
    "# # Reorder by descending sequence length\n",
    "train_seq_times = train_seq_times[reorder_indices_train]\n",
    "train_seq_types = train_seq_types[reorder_indices_train]\n",
    "#\n",
    "dev_seq_lengths, reorder_indices_dev = dev_seq_lengths.sort(descending=True)\n",
    "# # Reorder by descending sequence length\n",
    "dev_seq_times = dev_seq_times[reorder_indices_dev]\n",
    "dev_seq_types = dev_seq_types[reorder_indices_dev]\n",
    "\n",
    "test_seq_lengths, reorder_indices_test = test_seq_lengths.sort(descending=True)\n",
    "# # Reorder by descending sequence length\n",
    "test_seq_times = test_seq_times[reorder_indices_test]\n",
    "test_seq_types = test_seq_types[reorder_indices_test]\n",
    "\n",
    "max_sequence_length = max(train_seq_lengths[0], dev_seq_lengths[0], test_seq_lengths[0])\n",
    "print('max_sequence_length: {}'.format(max_sequence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T22:09:36.573951Z",
     "start_time": "2021-06-23T22:09:36.366652Z"
    }
   },
   "outputs": [],
   "source": [
    "model = train_sahp.make_model(max_sequence_length=max_sequence_length+1, process_dim=3)\n",
    "model_dict =torch.load('replicated_models/sahp-retweet_hidden16-20210623-055702',map_location=torch.device('cpu'))\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "batch_size = 32\n",
    "test_size = test_seq_times.size(0)\n",
    "device = 'cpu'\n",
    "test_loop_range = list(range(0, test_size, batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T22:11:22.439632Z",
     "start_time": "2021-06-23T22:09:49.489136Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:57<00:00, 34.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 27389707.904998768\n",
      "Type prediction score: 0.5155\n",
      "tensor(4.2995)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Get test Loss\n",
    "test_event_num, epoch_test_loss = eval_sahp(batch_size, test_loop_range, test_seq_lengths, test_seq_times,\n",
    "                                            test_seq_types, model, device, 0)\n",
    "\n",
    "test_loss = epoch_test_loss/test_event_num\n",
    "\n",
    "## Get Result Metrics\n",
    "avg_rmse, types_predict_score, results = prediction_evaluation(\n",
    "    device, model, test_seq_lengths, test_seq_times, test_seq_types, test_size, tmax)\n",
    "\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T22:27:37.599244Z",
     "start_time": "2021-06-23T22:27:37.584277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234.04366"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean(incr_errors[keep_indices]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T03:51:48.073185Z",
     "start_time": "2021-07-28T03:51:47.915135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample size: 4000\n",
      "Train sample size: 3200/4000\n",
      "Dev sample size: 400/4000\n",
      "No. of event tokens in training subset: tensor(498611)\n",
      "No. of event tokens in development subset: tensor(63349)\n",
      "No. of event tokens in test subset: tensor(61657)\n",
      "max_sequence_length: 323\n",
      "the number of trainable parameters: 4186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SAHP(\n",
       "  (gelu): GELU()\n",
       "  (type_emb): TypeEmbedding(3, 16, padding_idx=2)\n",
       "  (position_emb): BiasedPositionalEmbedding(\n",
       "    (Wt): Linear(in_features=1, out_features=8, bias=False)\n",
       "  )\n",
       "  (attention): MultiHeadedAttention(\n",
       "    (linear_layers): ModuleList(\n",
       "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (1): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (output_linear): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (attention): Attention()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (feed_forward): PositionwiseFeedForward(\n",
       "    (w_1): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (w_2): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (activation): GELU()\n",
       "  )\n",
       "  (input_sublayer): SublayerConnection(\n",
       "    (norm): LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (output_sublayer): SublayerConnection(\n",
       "    (norm): LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (start_layer): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (1): GELU()\n",
       "  )\n",
       "  (converge_layer): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (1): GELU()\n",
       "  )\n",
       "  (decay_layer): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (1): Softplus(beta=10.0, threshold=20)\n",
       "  )\n",
       "  (intensity_layer): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=2, bias=True)\n",
       "    (1): Softplus(beta=1.0, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_dim = 2\n",
    "device = 'cpu'\n",
    "train_ratio = 0.8\n",
    "lr = 5e-5\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "with open('data/simulated/hawkes_synthetic_random_2d_20191130-180837.pkl', 'rb') as f:\n",
    "    loaded_hawkes_data = pickle.load(f)\n",
    "    \n",
    "process_dim = loaded_hawkes_data['process_dim'] if 'process_dim' in loaded_hawkes_data.keys() else process_dim\n",
    "\n",
    "seq_times, seq_types, seq_lengths, _ = process_loaded_sequences(loaded_hawkes_data, process_dim)\n",
    "\n",
    "seq_times = seq_times.to(device)\n",
    "seq_types = seq_types.to(device)\n",
    "seq_lengths = seq_lengths.to(device)\n",
    "\n",
    "total_sample_size = seq_times.size(0)\n",
    "print(\"Total sample size: {}\".format(total_sample_size))\n",
    "\n",
    "train_size = int(train_ratio * total_sample_size)\n",
    "dev_ratio = 0.1\n",
    "dev_size = int(dev_ratio * total_sample_size)\n",
    "print(\"Train sample size: {:}/{:}\".format(train_size, total_sample_size))\n",
    "print(\"Dev sample size: {:}/{:}\".format(dev_size, total_sample_size))\n",
    "\n",
    "# Define training data\n",
    "train_seq_times = seq_times[:train_size]\n",
    "train_seq_types = seq_types[:train_size]\n",
    "train_seq_lengths = seq_lengths[:train_size]\n",
    "print(\"No. of event tokens in training subset:\", train_seq_lengths.sum())\n",
    "\n",
    "# Define development data\n",
    "dev_seq_times = seq_times[train_size:train_size + dev_size]  # train_size+dev_size\n",
    "dev_seq_types = seq_types[train_size:train_size + dev_size]\n",
    "dev_seq_lengths = seq_lengths[train_size:train_size + dev_size]\n",
    "print(\"No. of event tokens in development subset:\", dev_seq_lengths.sum())\n",
    "\n",
    "test_seq_times = seq_times[-dev_size:]\n",
    "test_seq_types = seq_types[-dev_size:]\n",
    "test_seq_lengths = seq_lengths[-dev_size:]\n",
    "\n",
    "print(\"No. of event tokens in test subset:\", test_seq_lengths.sum())\n",
    "\n",
    "## sequence length\n",
    "train_seq_lengths, reorder_indices_train = train_seq_lengths.sort(descending=True)\n",
    "# # Reorder by descending sequence length\n",
    "train_seq_times = train_seq_times[reorder_indices_train]\n",
    "train_seq_types = train_seq_types[reorder_indices_train]\n",
    "#\n",
    "dev_seq_lengths, reorder_indices_dev = dev_seq_lengths.sort(descending=True)\n",
    "# # Reorder by descending sequence length\n",
    "dev_seq_times = dev_seq_times[reorder_indices_dev]\n",
    "dev_seq_types = dev_seq_types[reorder_indices_dev]\n",
    "\n",
    "test_seq_lengths, reorder_indices_test = test_seq_lengths.sort(descending=True)\n",
    "# # Reorder by descending sequence length\n",
    "test_seq_times = test_seq_times[reorder_indices_test]\n",
    "test_seq_types = test_seq_types[reorder_indices_test]\n",
    "\n",
    "max_sequence_length = max(train_seq_lengths[0], dev_seq_lengths[0], test_seq_lengths[0])\n",
    "print('max_sequence_length: {}'.format(max_sequence_length))\n",
    "\n",
    "\n",
    "d_model = 16\n",
    "atten_heads = 1\n",
    "dropout = 0.1\n",
    "\n",
    "model = make_model(nLayers=1, d_model=d_model, atten_heads=atten_heads,\n",
    "                   dropout=dropout, process_dim=process_dim, device=device, pe='add',\n",
    "                   max_sequence_length=max_sequence_length + 1).to(device)\n",
    "\n",
    "print(\"the number of trainable parameters: \" + str(count_parameters(model)))\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9, weight_decay=3e-4)\n",
    "model_opt = NoamOpt(d_model, 1, 100, initial_lr=lr, optimizer=optimizer)\n",
    "\n",
    "\n",
    "## Size of the traing dataset\n",
    "train_size = train_seq_times.size(0)\n",
    "dev_size = dev_seq_times.size(0)\n",
    "test_size = test_seq_times.size(0)\n",
    "tr_loop_range = list(range(0, train_size, batch_size))\n",
    "de_loop_range = list(range(0, dev_size, batch_size))\n",
    "test_loop_range = list(range(0, test_size, batch_size))\n",
    "\n",
    "last_dev_loss = 0.0\n",
    "early_step = 0\n",
    "\n",
    "random_seeds = list(range(0, 1000))\n",
    "random.shuffle(random_seeds)\n",
    "\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T04:03:13.623046Z",
     "start_time": "2021-07-28T04:03:13.523347Z"
    }
   },
   "outputs": [],
   "source": [
    "i_batch = 0\n",
    "model_opt.optimizer.zero_grad()\n",
    "\n",
    "batch_onehot, batch_seq_times, batch_dt, batch_seq_types, _, _, _, batch_seq_lengths = \\\n",
    "    get_batch(batch_size, i_batch, model, train_seq_lengths, train_seq_times, train_seq_types,\n",
    "                   rnn=False)\n",
    "\n",
    "batch_seq_types = batch_seq_types[:, 1:]\n",
    "\n",
    "masked_seq_types = MaskBatch(batch_seq_types, pad=model.process_dim,\n",
    "                             device=device)  # exclude the first added even\n",
    "model.forward(batch_dt, masked_seq_types.src, masked_seq_types.src_mask)\n",
    "# nll = model.compute_loss(batch_seq_times, batch_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T04:22:55.043265Z",
     "start_time": "2021-07-28T04:22:55.020327Z"
    }
   },
   "outputs": [],
   "source": [
    "### Loss Function\n",
    "def compute_loss(model,seq_times,seq_onehot_types,n_mc_samples = 20):\n",
    "    dt_seq = seq_times[:, 1:] - seq_times[:, :-1]\n",
    "    cell_t = model.state_decay(model.converge_point, model.start_point, model.omega, dt_seq[:, :, None])\n",
    "\n",
    "    n_batch = seq_times.size(0)\n",
    "    n_times = seq_times.size(1) - 1\n",
    "    device = dt_seq.device\n",
    "    # Get the intensity process\n",
    "    intens_at_evs = model.intensity_layer(cell_t)\n",
    "    \n",
    "    \n",
    "    log_intensities = intens_at_evs.log()  # log intensities\n",
    "    log_intensities =  log_intensities*seq_onehot_types[:, 1:, :].sum(dim=-1).unsqueeze(-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    seq_mask = seq_onehot_types[:, 1:]\n",
    "    log_sum = (log_intensities * seq_mask).sum(dim=(2, 1)) \n",
    "    \n",
    "    \n",
    "    taus = torch.rand(n_batch, n_times, 1, n_mc_samples).to(device) \n",
    "    taus = dt_seq[:, :, None, None] * taus \n",
    "    \n",
    "    \n",
    "    cell_tau = model.state_decay(\n",
    "    model.converge_point[:, :, :, None],\n",
    "    model.start_point[:, :, :, None],\n",
    "    model.omega[:, :, :, None],\n",
    "    taus)\n",
    "    \n",
    "    cell_tau = cell_tau.transpose(2, 3)\n",
    "    intens_at_samples = model.intensity_layer(cell_tau).transpose(2, 3)\n",
    "    intens_at_samples = intens_at_samples*seq_onehot_types[:, 1:, :].sum(dim=-1).unsqueeze(-1).unsqueeze(-1)\n",
    "    \n",
    "    \n",
    "    total_intens_samples = intens_at_samples.sum(dim=2)  # shape batch * N * MC\n",
    "    partial_integrals = dt_seq * total_intens_samples.mean(dim=2)\n",
    "\n",
    "    integral_ = partial_integrals.sum(dim=1)\n",
    "\n",
    "    res = torch.sum(- log_sum + integral_)\n",
    "    \n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Other 2-D Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T04:26:54.485162Z",
     "start_time": "2021-07-28T04:26:54.429348Z"
    }
   },
   "outputs": [],
   "source": [
    "process_dim = 2\n",
    "device = 'cpu'\n",
    "train_ratio = 0.8\n",
    "lr = 5e-5\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "with open('data/simulated/hawkes_2d.pkl', 'rb') as f:\n",
    "    loaded_hawkes_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T04:27:03.049518Z",
     "start_time": "2021-07-28T04:27:03.044531Z"
    }
   },
   "outputs": [],
   "source": [
    "process_dim = loaded_hawkes_data['process_dim'] if 'process_dim' in loaded_hawkes_data.keys() else process_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-28T04:27:06.097620Z",
     "start_time": "2021-07-28T04:27:06.090606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
